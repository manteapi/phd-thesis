=====================================================================================
                                    Introduction
=====================================================================================

Bonjour à tous.
Je suis Pierre-Luc Manteaux et je vais vous présenter les travaux réalisés pendant ma thèse,
intitulée "Simulation et contrôle de phénomènes physiques", qui a été supervisé par François
Faure et Marie-Paule Cani, au sein de l'équipe IMAGINE du laboratoire Jean Kuntzmann.

Cette thèse s'inscrit dans le domaine de l'informatique graphique que beaucoup de personnes 
associent au cinéma d'animation. Je vais profiter de cet exemple pour introduire ce domaine.

Jusqu'au milieu des années 80, chaque image d'un dessin animé est dessinée, coloriée à la main.
Pour un long métrage de 90 minutes, ça représente 135000 images sans compter les ébauches.
C'est un travail colossale mais qui en vaut la chandelle. 
Le succès est là, ça créé des bénéfices et tout une industrie se créé autour.

A cette période, l'informatique se démocratise.
Les principales étapes de création d'un dessin animé sont réalisées à l'aide d'ordinateurs.
Ça permet d'économiser un temps considérable sur des tâches répétitives(interpolation,colorisation).
Sans brider l'imagination et la capacité à transmettre des émotions.
De nouveaux outils émergent, qui permettent de créer des animations toujours plus spectaculaires.

Un de ces outils, qui va devenir indispensable, c'est la simulation physique.
Qui fait partie des domaines dans lesquels s'inscrit ma thèse.
Elle va permettre de créer automatiquement des animations complexes et réalistes.
Des animations qui auraient été très difficile, voire quasi-impossible à obtenir par le dessin.
On va même pouvoir la détourner pour intégrer des lois non pas physique mais artistique.
Et ainsi créer des animations combinant le réalisme d'une simulation physique et l'expressivité d'une intention artistique.

Au delà du cinéma, on retrouve l'IG et la SP dans d'autres applications pour le divertissement (JV).
Mais aussi dans d'autres domaines, comme l'enseigment (simulateur), et la fabrication (design).
D'une part, car ils permettent de simuler et visualiser des comportements réalistes/complexes.
Le réalisme dépend bien sûr des objectifs de l'application.
On aura pas les même attentes d'une simulation entre un jeu vidéo et un protocole de chirurgie.
D'autre part, car ils permettent d'intéragir avec ces phénomènes.
Les environnements virtuels deviennent plus immersifs.
Les processus de design d'objets s'accélèrent.

Pour que ces applications remplissent leurs objectifs, 
il y a plusieurs défis auxquels la simulation physique doit faire face. 
Je vais en introduire trois qui font échos aux travaux de ma thèse.

Un des premiers défis qu'on rencontre, c'est celui de la complexité calculatoire.
Une simulation nécessite des ressources informatiques importantes (mémoire, unité de calcul).
Pour illustrer, j'ai pris une image d'un simulateur de liquide, dans lequel un bloc d'eau est lâché dans un bassin qui contient un obstacle. 
L'eau est représentée à l'aide de particules, ici 6 millions de particules, qui représentent chacune un petit volume d'eau qui intéragit avec ses voisins en suivant des équations qui décrivent l'écoulement d'un liquide. 
L'implémentation du simulateur, elle,  est loin d'être naïve, elle fait appel aux capacités de calcul de plusieurs cartes graphiques qui intéragissent pour équilibrer au mieux la charge de calcul. 
Au final, il faut 2 heures et demi pour simuler 53 secondes de temps réel. 

Le second défi, lui, est relatif à la description des phénomènes que l'on cherche à simuler.
Aujourd'hui on est capable de représenter de manière efficace des objets rigides, déformables, des fluides ou des liquides et donc on les retrouve fréquemment dans des environnements intéractifs grand publics : jeux vidéos, logiciel d'animation. 
En revanche, ce que l'on trouve moins, ce sont des phénomènes qui mettent en jeu des changements de formes et d'état important: on peut penser aux changements de phases d'un liquide, aux grandes déformations d'un objet visqueux, ou à la découpe/fracture d'objets déformables. 
Là il y a un besoin de modèles numériques qui permettent de simuler efficacement ces phénomènes complexes pour que l'on puisse les retrouver dans des contextes intéractifs.

Le troisième défi concerne le design et le contrôle de simulation.
Il y a plein de logiciels qui permettent de designer une simulation en laissant à l'utilisateur la main sur un certains nombre de paramètres : conditions aux bords, conditions initiales, paramètres matériaux. 
Par exemple le logiciel houdini, sur l'image, qui est utilisé par dreamworks dans ses productions. Donc ça marche. 
Quand on utilise ces logiciels et que l'on a une idée précise de ce que l'on souhaite obtenir, il y a trois difficultés principales. 
La première c'est la courbe d'apprentissage du logiciel, l'expertise qui est attendu de l'utilisateur sur les modèles physiques qui sont utilisés pour définir les paramètres et l'interface qui peut devenir écrasante. 
La seconde c'est le contrôle indirect que l'on a sur la simulation, on doit se débrouiller avec les conditions aux bords et les conditions initiales. C'est un savant jeu de relations de cause à effets. 
Enfin, c'est le temps de calcul, qui va restraindre l'exploration des paramètres de la simulation à la patience de l'utilisateur. Donc, concevoir et contrôler des simulations restent très fastidieux, surtout quand on est neophytes, et il y a un besoin d'outils simple, direct et intuitifs pour le contrôle de simulations.

Ok, ce sont les trois défis autour desquelles s'articulent les contributions de cette thèse que je vais maintenant présenter. 

Tout d'abord, pour répondre au défi sur l'efficacité des simulations, on a étudié les modèles adaptatifs existants en informatique graphique et on a introduit un nouveau modèle pour la simulation de liquide et de vêtements à l'aide de particules. C'est un modèle qui essaie de répondre à certaines limitations courantes des modèles adaptatifs existant tout en proposant un compromis précision/performance intéressant.

Ensuite, on s'est intéressé à la simulation de découpe. On propose une technique qui permet de simuler de manière intéractive des découpes très détaillées d'objets fins.

Enfin, sur le contrôle de simulation, on propose une nouvelle approche qui consiste à éditer des animations de liquide existantes à l'aide d'outils inspirés de la sculpture virtuelle.

=====================================================================================
                                        ARPS
=====================================================================================

Je vais commencer par introduire les modèles physiques adaptatifs pour l'informatique graphique.

Comme je le disais précédemment, une simulation physique peut coûter très cher en temps de calcul.
Pour schématiser, plus ce que l'on cherche à simuler est complexe et doit être proche de la réalité, plus le coût sera important. 
Tout simplement parce qu'il faudra alors avoir un échantillonnage de noeuds de calculs suffisamment dense pour résoudre toute la complexité de la dynamique. 
Les trois images qui sont ici illustrent bien ça. Peu importe que ce soit des triangles, des tétrahèdres ou des particules, on a besoin d'un échantillonnage suffisamment dense pour pouvoir capturer les plis d'un vêtement, les rides d'un visage ou les détails présent à la surface de l'eau.
Et ça va résulter en un temps de calcul important.

Une approche générale pour réduire ce temps de calcul, c'est d'utiliser des techniques adaptatives.
Ça consiste à modifier les différents composants de la simulation et la représentation mathématiques de l'objet au cours de la simulation pour obtenir le meilleur compromis entre précision, performance et stabilité.

Une grande partie de ces méthodes consistent à adapter dyn la discrétisation spatiale et temporelle.
E.g, pour la discrétisation spatiale, ça va consister à rajouter des degrés de libertés dans les zones qui subissent de grandes déformations, à en enlever dans les régions qui sont éloignés de la caméra et qui présentent un intérêt visuel faible.
Pour la discrétisation temporelle, ça consister à adapter le pas de temps de la simulation aux événements. Des événements très rapide comme des chocs vont nécessiter un pas de temps plus petit pour garantir la stabilité de la simulation alors que des événements lents pourront être simulé avec un pas de temps plus important et donc plus rapidement.

En complément de ces deux grandes stratégies, il y a tout un panel de méthodes adaptatives que l'on peut étudier et ranger dans différentes catégories.
Je vais pas les détailler ici, je vais juste m'arrêter sur des limitations communes à un grand nombre de ces méthodes et qui peuvent freiner leur utilisation.
De prime abord, on imagine une méthode adaptative comme une sorte de boîte noire que l'on va pouvoir rajouter sur un modèle existant dans un simulateur donné pour le rendre adaptatif.
En pratique c'est rarement le cas, c'est extrêmement difficile d'avoir un simulateur capable d'intégrer à la fois des modèles physiques différents de manière unifié et qui puisse accueillir des modules d'adaptativité sans remettre l'architecture du simulateur en question. 
Il y a une difficulté qui est lié au côté intrusif de ces méthodes et qui du coup va compliquer également la ré-utilisation d'implémentation existante dans d'autres simulateurs. 
C'est encore plus vrai quand on souhaite combiner une technique adaptative avec d'autres techniques d'accélérations comme le GPU où la on peut rencontrer des incompatibilités.

Maintenant je vais vous présenter le modèle que l'on propose pour répondre en partie à ces limites.
Si je devais le classer dans cet arbre, il serait dans la famille des méthodes de freezing (rouge).

Ces méthodes de freezing ont pour objectif de gagner du temps dans ces situtations quasi-statiques.
Ces situations, elles sont nombreuses. On peut penser à tout les scénarios de contact locaux.
Comme l'intéraction entre la sphère et le canapé, ou alors entre l'outil et l'organe dans un sim.
Pour les liquides on peut penser aux eaux profondes/stagnantes.
Et on peut penser à des matériaux qui présente de la friction et qui vont s'arrêter rapidement.
Ce que l'on veut dans ces situations, c'est concentrer les calculs dans les zones intéressantes.
En mouvement, et économiser du temps dans les zones immobiles.

C'est des méthodes que l'on retrouve dans les moteurs de jeux vidéos pour les objets rigides.
Côté recherche, on trouve surtout des travaux concernant les rigides et rigides articulées.
Pour les fluides, il n'y a qu'un seul travail à ma connaissance qui propose une méthode.
Et on n'en n'a pas répertorié pour la simulation d'objets déformables.

Pour les rigides, il y a deux travaux qui proposent d'utiliser le freezing pour accélerer le traitements des collisions dans des scénarios d'empilement.
Les objets immobiles et en contact sont considérés comme inactif, il n'y a aucun traitement des contacts pour ces groupes. 
Lors d'une collision avec un object actif, les objets inactifs sont réactivées par propagation pour prendre en compte les contacts.

Pour les rigides articulées, Kim et al. propose de activé/désactivé certains joints en fonction de critères comme la distance à la caméra pour simplifier les objets et réduire la complexité.

Pour les fluides Goswami et al. propose une méthode pour la simulation particulaire de liquide.
Les particules qui sont lentes sont approximées par des particules immobiles pour réduire le temps de calcul. Malheureusement le principe d'action/réaction n'est pas respecté et donc le moment n'est pas conservé.

Dans tout ces travaux, la question de comment/quand réactiver les objets/particules inactives de manière cohérente n'est pas addréssée ou alors suppose l'utilisation d'heuristiques spécifiques à des applications.

En 2012, dans le domaine de la dynamique moléculaire.
Artemova & Redon ont proposé une approche qui répond à cette question de manière élégante.
Ils l'ont intitulé, Adaptively Restrained Particle Simulations (ARPS).
Comme Goswami et al., les particules lentes sont approximées par des particules immobiles.
Ce qui résulte en des forces inter-particules constantes, qui n'ont plus besoin d'être calculées.
Toute la différence est dans le fait que la quantité de mouvement associée à ces forces constantes continuent d'être accumulés pour permettre aux particules de reprendre un mouvement qui soit physiquement cohérent.
La méthode a été validée sur différents exemples, dont celui d'une collision en cascade(bas,droite).
Une particule a été projeté contre un réseau 2D de particules.
La couleur correspond au déplacement des particules.
On peut observer l'onde de choc propagée à la suite de la collision collision.
La simulation adaptative est 5x plus rapide que la simulation de référence.
Et on peut observer que les caractéristiques du choc ont été très bien préservées.
La méthode intègre deux seuils que l'utilisateur peut modifier pour préciser un compromis entre precision/performance.

Nos contributions se limitent à l'étude et à l'extension de l'ARPS pour l'informatique graphique.
Tout d'abord dans le cadre de la simulation de liquide où on propose un algorithme qui permet de combiner l'ARPS avec le modèle SPH et d'en tirer des accélérations significatives.
Ensuite dans le cadre de la simulation de vêtement, où on a dérivé un intégrateur implicite qui s'appuie sur l'ARPS pour permettre l'utilisation de pas de temps important.
Dans la suite de cette partie, je vais rappeler comment fonctionne l'ARPS.
Ensuite je présenterais les deux contributions et terminerais avec des limitations et perspectives.

Voici une courte vidéo pour mieux illustrer le principe de l'ARPS.
Sur la gauche un oscillateur harmonique, deux particules reliés par un ressort, une particule fixe.
Sur la droite le protrait de phase de la simulation qui décrit l'évolution de la quantité de mouvement en fonction de la position de la particule. 
Pendant une simulation classique, la force inter-particule est calculée à chaque pas de temps,
même si rien d'intéressant ne se passe ou que la quantité de mouvement est faible.
Maintenant, le même système mais simulé avec l'ARPS.
Le portrait de phase est légèrement différent.
Il est divisée en trois régions qui décrivent les différentes dynamiques(restr., trans., classique.)
A ces dynamiques, correspond l'état de la particule, qui peut être inactive, trans., active.
Pendant la dynamique restrainte, la particule est immobile mais accumule de la quantité de mouvement jusqu'à un certain seuil.
Alors la particule devient transitive.
L'objectif de cette phase est de dépensée l'énergie accumulée pendant la dynamique restreinte et retrouver une dynamique classique, la particule est alors active.
Le temps de calcul est économisé pendant la dynamique restreinte où la force interparticule n'a pas à être calculé à chaque pas de temps.
Et donc, en utilisant des seuils adaptés pour la zone de transition, on peut avoir une simulation adaptative qui reste proche de la simulation de référence.

L'ARPS c'est avant tout un intégrateur qui permet de faire suivre aux degrés de liberté une dynamique restreinte ou classique en fonction de leur quantité de mouvement. A partir de ça, on peut imaginer des algorithmes qui en tire partie de la dynamique restreinte pour réduire les calculs.
C'est ce que l'on a fait avec notre extension à la simulation de fluide particulaires.

On a choisi le modèle SPH, Smoothed Particles Hydrodynamics, qui est courant en IG et qui semblait bien convenir pour appliquer l'ARPS.
SPH consiste à interpoler les quantités d'un fluide(densité, vitesse, ...) via des particules.
L'algorithme classique est le suivant.
Pour chaque particule on va déterminer ses voisins.
C'est le gouleau d'étranglement de l'algorithme.
Ensuite de manière générale on va calculer les champs scalaires (pression, densité) 
et les forces inter-particules (pression, viscosité).
Finalement, on intègre les particules dans le temps.

Comme je disais, la recherche des voisins correspond à l'étape la plus couteuse de l'algorithme.
En combinant SPH avec l'ARPS, on va pouvoir réduire la recherche des voisins aux particules actives/transitives et le calcul des scalaires/forces aux particules actives/transitives et à leurs voisins.
Le dernier point est possible car toutes les contributions entre particules sont symétriques.
On arrive à un algorithme qui exploite au maximum l'inactivité des particules pour économiser du temps. Il est le suivant.

On commence par soustraire les anciennes contributions des particules actives et de leur voisins.
Ensuite on va calculer les voisins uniquement pour les particules actives/transitives.
On va de nouveau calculer les champs scalaires/forces mais uniquement pour les particules actives.
Enfin on intègre le mouvement de chaque particule à l'aide de l'ARPS et on met à jour l'état de particule en fonction de leur quantité de mouvement.
A la fin de l'algorithme, on a calculé uniquement ce qui était nécessaire et on a respecté le principe d'action/réaction.

Dans cette combinaison entre l'ARPS et le modèle SPH, il y a un point sur lequel on a buté.
Ça concerne les forces de viscosités, qui impliquent non seulement les positions des particules, 
mais également leur vitesse, et ce qui n'était pas le cas d'utilisation prévu par Artemova & Redon.
Avec cette méthode on a deux notions de vitesses à prendre en compte.
La vitesse accumulée qui définit l'état des particules et la vitesse effective qui est utilisée pour intégrer les positions des particules et qui prend en compte leur état.
Alors bien sûr c'est la vitesse effective qui doit être utilisé lors du calcul des forces de viscosité de part son adéquation avec l'état de la particule. 
Dans nos expériences, on a constaté un effet de bord que l'on ne sait pas expliqué clairement.
La force de viscosité disparaît avec la vitesse effective et fait converger l'état de la particule vers l'inactivité sans jamais l'atteindre.
Les particules bougent à peine mais aucune économie n'est faite car elles sont considérées comme active.
Pour résoudre rapidement ce problème, on a ajouté un seuil supplémentaire pour empêcher ces situations mais ça reste un point à éclaircir dans des travaux futurs.

Voici quelques résultats. 

Tout d'abord une simulation d'un dam break où l'on compare SPH et l'ARPS.
Ici la visualisation de l'état des particules. 
Pour cette simulation on a obtenu une accélération moyenne de 3.8.
On peut constate que l'on a un comportement très proche d'une simulation classique.

Et un second exemple dans lequel on a créé un courant permanent.
Une fois que le flux est installé, un grand nombre de particules bougent très peu mais réagissent aux intéractions avec les particules actives.
L'accélération moyenne est de 2.7 et on voit bien dans cet exemple que l'activation/la désactivation des particules est bien gérée par l'algorithme.

Passons maintenant à la simulation de vêtement.
Pour des objets structurés comme les vêtements, l'intégration explicite nécessite de très petit pas de temps et ralentit considérablement la simulation.
Une solution classique consiste à utiliser un intégrateur implicite comme celui proposé par Baraff et Witkin.
Les éq. du mouvement sont alors discrétisés en considérant les forces à la fin du pas de temps.
En utilisant un développement de Taylor Young, on arrive à un système linéaire qu'il faut résoudre.
La résolution est cher mais permet d'utiliser un pas de temps beaucoup plus grand.
Notre idée ici est de dériver un intégrateur implicite pour l'ARPS pour réduire la taille du système linéaire en accord avec le nombre de particules actives.

On est tout simplement parti des équations du mouvement décrite par l'ARPS.
On a suivi la même procédure que je viens de décrire : discrétisation, développement taylor.
On obtient un système linéaire similaire à celui que l'on avait mais qui contient 2 nouveaux termes.
Une matrice et un vecteur qui servent à encapsuler l'état des particules.
On peut observer que lorsque les particules sont inactives, on retrouve une intégration explicite.
Alors que lorsque les particules sont actives on retrouve l'intégration implicite.
Donc on peut extraire les particules inactives du système linéaire et ainsi réduire sa taille.

On a implémenté ce solveur hybride sur un cas extrêmement simple d'un vêtement qui tombe sous l'effet de la gravité.
Dans cet exemple, les particules inactives sont intégrées explicitement, les particules actives sont intégrées implicitement et on a bien une réduction du système linéaire qui permet d'obtenir une accélération de 2.7.

Voilà pour cette première partie, on a étendu à l'informatique graphique une nouvelle approche pour approximer de manière cohérente des simulations de particules.
On l'a fait de deux manières, via une application aux fluides et via une application aux vêtements.
Dans les deux cas, on a obtenu des accélérations encourageantes.
La méthode n'est pas sans limite, on a observé des instabilités concernant l'intégration implicite, qui viennent de la fonction de restriction utilisée pour gérer la transition des particules.
Ça nécessiterait de mieux comprendre l'origine du problème pour pouvoir apporter des solutions.
Sinon, ça serait également intéressant d'étudier l'utilisation de critère visuel tel que la distance à la caméra pour déterminer l'état des particules et ainsi concentrer les ressouces là où elles contribuent le plus à l'intérêt visuel.
